# ğŸ“˜ **Phase 4: Deep Learning Foundations**  

## ğŸ¯ **Objective:**  
Develop a deep understanding of neural networks and how they power deep learning systems. Learn to build, train, and optimize models for computer vision, sequence processing, and foundational NLP.

## ğŸ” **Curriculum Breakdown**
---

### ğŸ§  **1. Neural Network Basics**

#### ğŸ“Œ 1.1 Core Concepts
- What are artificial neurons and how they mimic biological ones
- **Perceptrons** and the building blocks of neural networks
- **Activation functions**:
  - ReLU (default in hidden layers)
  - Sigmoid (binary classification)
  - Softmax (multi-class output layers)

#### ğŸ“Œ 1.2 Forward & Backward Propagation
- Forward pass: weighted sum and activation
- Backpropagation and the chain rule
- Gradient descent and parameter updates

#### ğŸ“Œ 1.3 Loss Functions
- **MSE (Mean Squared Error)**: for regression
- **Cross-Entropy**: for classification tasks
- Log loss interpretation and probabilities

ğŸ“Œ **Hands-On Lab**: Build a neural net from scratch using NumPy for a binary classification task

---

### ğŸ—ï¸ **2. Deep Learning Architectures**

#### ğŸ“Œ 2.1 Convolutional Neural Networks (CNNs)
- Convolutional layers, filters/kernels
- Feature maps and receptive fields
- Pooling (MaxPooling, AvgPooling)
- Flattening and fully connected layers
- Use-cases: image classification, object detection

#### ğŸ“Œ 2.2 Recurrent Neural Networks (RNNs) and LSTMs
- Time series and sequence modeling
- RNNs: vanishing gradients, sequential state
- LSTMs: forget/update gates, long-term memory
- Use-cases: stock prediction, text generation

#### ğŸ“Œ 2.3 Transformers (Intro Level)
- Limitations of RNNs â†’ Need for attention
- Self-attention and positional encoding
- Encoder-decoder overview
- Used in NLP, vision, audio â€” foundation for GenAI

ğŸ“Œ **Mini Project**: Build a CNN for classifying CIFAR-10 or MNIST  
ğŸ“Œ **Optional**: Sequence-to-sequence model for text input

---

### âš™ï¸ **3. Optimization & Regularization**

#### ğŸ“Œ 3.1 Optimization Algorithms
- **SGD**: classic baseline
- **Adam**: adaptive learning rates
- **RMSProp**: smoothing with momentum

#### ğŸ“Œ 3.2 Regularization Techniques
- **Dropout**: randomly turn off neurons during training
- **Batch Normalization**: normalize activations within layers
- **Weight Decay (L2 regularization)**: penalize large weights

#### ğŸ“Œ 3.3 Learning Rate Control
- Static vs. dynamic learning rates
- Learning rate schedulers
- Gradient clipping to avoid exploding gradients

ğŸ“Œ **Lab**: Train the same network with and without dropout, visualize differences in loss curves

---

### ğŸ§° **4. Frameworks & Tools**

#### âœ… Core Libraries
- **TensorFlow + Keras**:
  - High-level APIs, great for beginners
  - Deployment-ready
- **PyTorch**:
  - Dynamic graphs for flexible training loops
  - Preferred for research

#### âœ… Additional Ecosystem Tools
- **PyTorch Lightning**: modular code for cleaner experiments
- **Hugging Face Transformers**: use pre-trained models with ease
- **Weights & Biases / TensorBoard**: monitor experiments visually

ğŸ“Œ **Activity**: Build the same CNN in both TensorFlow and PyTorch

---

### ğŸ§ª **5. Training & Evaluation**

#### ğŸ“Œ 5.1 Model Training Workflow
- Batches, epochs, and early stopping
- Visualizing training vs. validation accuracy/loss
- Managing overfitting with validation sets and dropout

#### ğŸ“Œ 5.2 Preprocessing for Deep Learning
- Image:
  - Resizing, normalization, channel adjustments
- Text:
  - Tokenization, padding, vocab building, embeddings
- Data generators and input pipelines (tf.data, PyTorch Datasets)

ğŸ“Œ **Lab**: Build a training loop with live loss/accuracy chart

---

### ğŸ§  **6. Best Practices for Robust Training**

#### ğŸ“Œ 6.1 Data Augmentation
- For images: flipping, rotation, cropping, brightness
- For text: synonym replacement, random deletion, back-translation

#### ğŸ“Œ 6.2 Transfer Learning
- Load pre-trained models: VGG, ResNet, BERT
- Fine-tune vs. freeze layers
- Apply to small datasets with fewer resources

#### ğŸ“Œ 6.3 Checkpointing & Callbacks
- Save best model weights
- Auto-adjust learning rate on plateaus
- Stop early to prevent overfitting

ğŸ“Œ **Mini Project**: Train a CNN with augmentations and evaluate improvements  
ğŸ“Œ **Bonus**: Load a pre-trained model, fine-tune it on a custom dataset