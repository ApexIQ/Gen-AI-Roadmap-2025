# ðŸ“˜ **Phase 5: Core Generative AI Techniques**  

## ðŸŽ¯ **Objective:**  
Understand and experiment with the foundational techniques that power generative AI â€” including transformers, diffusion models, prompt design, multimodal architectures, and their evaluation.

## ðŸ” **Curriculum Breakdown**
---

### ðŸŒŸ **1. What is Generative AI?**

#### ðŸ“Œ 1.1 Core Definition
- Generative AI refers to models that can generate **original content** such as:
  - Text (e.g., articles, poems)
  - Images (e.g., AI art, photorealistic images)
  - Code (e.g., scripts, functions, APIs)
  - Audio/Music (e.g., background scores, voices)
  - Video (e.g., scene generation, motion tracking)

#### ðŸ“Œ 1.2 Key Differences
- Generative models output **novel** data, not just classifications or predictions
- Often based on probabilistic learning, latent space exploration, and learned representations

ðŸ§  *Mini-Reflection*: Contrast Linear Regression vs. GPT. Whatâ€™s being predicted?

---

### ðŸ¤– **2. Transformer Architectures**

#### ðŸ“Œ 2.1 Foundation Models
- **GPT (decoder-only)**: autoregressive, next-token prediction
- **BERT (encoder-only)**: masked token prediction for contextual embeddings
- **T5 (encoder-decoder)**: sequence-to-sequence multitask learning

#### ðŸ“Œ 2.2 Attention Mechanism
- Self-attention: each token looks at all other tokens
- Positional Encoding: adds order information
- Scaled Dot-Product Attention and Multi-head attention

#### ðŸ“Œ 2.3 Use-Cases
- Summarization, translation, QA, chat interfaces
- Few-shot learning and transfer across tasks

ðŸ“Œ **Lab**: Use Hugging Face Transformers to try GPT-2 and T5 on summarization and question answering

---

### ðŸŽ¨ **3. Diffusion Models**

#### ðŸ“Œ 3.1 Conceptual Workflow
- **Forward process**: add noise to data (e.g., images)
- **Reverse process**: learn how to denoise step-by-step
- Trained to reconstruct data from pure noise â€” effectively **generate from scratch**

#### ðŸ“Œ 3.2 Notable Models
- **Stable Diffusion**
- **DALLÂ·E**, **MidJourney**
- Open-source pipelines via Hugging Face Diffusers library

#### ðŸ“Œ 3.3 Applications
- Text-to-image generation
- Inpainting: filling in masked image regions
- Style transfer and enhancement

ðŸ“Œ **Exercise**: Generate 5 unique images using Stable Diffusion + custom prompts

---

### ðŸ§  **4. Prompt Engineering**

#### ðŸ“Œ 4.1 Prompting Styles
- **Zero-shot**: "What is the capital of France?"
- **Few-shot**: Provide examples for structure
- **Chain-of-Thought**: encourage reasoning step-by-step

#### ðŸ“Œ 4.2 Prompt Design Tips
- Be specific about format, tone, persona
- Use delimiters, examples, and system messages

#### ðŸ“Œ 4.3 Tools for Prompting
- **OpenAI Playground**
- **Hugging Face Inference API**
- Prompt IDEs and testing platforms (PromptLayer, FlowGPT)

ðŸ“Œ **Lab**: Design three prompts to achieve the same goal with different styles (reasoning, creativity, structure)

---

### ðŸ§¬ **5. Multimodal AI**

#### ðŸ“Œ 5.1 What is Multimodal Learning?
- Models that understand and generate across **multiple data types** (text, image, audio)

#### ðŸ“Œ 5.2 Notable Multimodal Models
- **CLIP**: connects text and images via embeddings
- **BLIP**, **BLIP-2**: vision-language transformers
- **GPT-4V (Vision)**: image + language prompts
- **Flamingo**, **Gemini**: text, image, video, interleaved understanding

#### ðŸ“Œ 5.3 Use-Cases
- Visual question answering
- Document parsing and captioning
- Image-grounded conversational agents

ðŸ“Œ **Demo**: Use Hugging Face Spaces for image captioning with BLIP-2

---

### ðŸ“ **6. Evaluation Metrics in GenAI**

#### ðŸ“Œ 6.1 Text Metrics
- **BLEU**: overlap in n-grams (machine translation)
- **ROUGE**: recall-based overlap for summarization
- **METEOR**: synonym-aware evaluation

#### ðŸ“Œ 6.2 Image Metrics
- **FID (FrÃ©chet Inception Distance)**: visual realism and diversity
- **Inception Score (IS)**: quality and uniqueness

#### ðŸ“Œ 6.3 Human Evaluation
- Coherence, fluency, creativity
- Alignment with instructions and safety

ðŸ“Œ **Activity**: Evaluate multiple GenAI outputs using BLEU + human review side-by-side

---

### ðŸ”® **7. Model Capabilities Showcase**

| Task | Model Examples |
|------|----------------|
| **Text Generation** | ChatGPT, Claude, Mistral |
| **Code Generation** | Codex, GitHub Copilot |
| **Image Generation** | DALLÂ·E 3, MidJourney, SDXL |
| **Music/Audio** | MusicLM, AudioCraft, Riffusion |
| **Video (emerging)** | Runway, Pika, Sora (OpenAI) |

ðŸ“Œ **Challenge**: Use 3 different GenAI APIs or models (text, image, code) in one mini-project

---

### ðŸ§° Tools & Libraries

- **Hugging Face Transformers, Diffusers**
- **OpenAI APIs (ChatGPT, DALLÂ·E)**
- **Replicate, Stability AI, DreamStudio**
- **PromptLayer, LangChain (prompt chaining)**
- **Colab + Gradio for quick interfaces**

---

### ðŸ§ª Capstone Mini-Project

> **Project**: *Build a Prompt-Driven Generative App*  
- Choose a domain: text, code, image  
- Design creative prompts + use 1â€“2 APIs or models  
- Build a simple front-end using Gradio or Streamlit  
- Include evaluation (automated or human) and output examples  
- Push to GitHub + share on Hugging Face Spaces (optional)